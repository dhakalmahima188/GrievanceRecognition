{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 20:57:50.982382: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-10 20:57:50.982419: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-10 20:57:50.982970: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-10 20:57:51.080420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 20:57:52.197856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"dataset/updated_text_data.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=df[df['गुनासो वर्ग']=='वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी ']\n",
    "df_2=df[df['गुनासो वर्ग']=='सोधपुछ, सुझाव, प्रशंसा सम्बन्धी'].sample(2863,replace=True)\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "df_3=df[df['गुनासो वर्ग']=='कर्मचारी सम्वन्धी '].sample(2863,replace=True)\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "df_4=df[df['गुनासो वर्ग']=='स्वास्थ्यसँग सम्बन्धी'].sample(2863,replace=True)\n",
    "df_4 = df_4.reset_index(drop=True)\n",
    "df_5=df[df['गुनासो वर्ग']=='अर्थ सबन्धी '].sample(2863,replace=True)\n",
    "df_5 = df_5.reset_index(drop=True)\n",
    "df_6=df[df['गुनासो वर्ग']=='खानेपानी सम्बन्धी '].sample(2863,replace=True)\n",
    "df_6 = df_6.reset_index(drop=True)\n",
    "df_7=df[df['गुनासो वर्ग']=='सूचना तथा  संचार सम्बन्धी'].sample(2863,replace=True)\n",
    "df_7 = df_7.reset_index(drop=True)\n",
    "df_8=df[df['गुनासो वर्ग']=='शान्ति सुरक्षा सम्बन्धी '].sample(2863,replace=True)\n",
    "df_8 = df_8.reset_index(drop=True)\n",
    "df_9=df[df['गुनासो वर्ग']=='प्राकृतिक श्रोत/साधन सम्बन्धी'].sample(2863,replace=True)\n",
    "df_9 =df_9.reset_index(drop=True)\n",
    "df_10=df[df['गुनासो वर्ग']=='लागु पदार्थ सम्बन्धी '].sample(2863,replace=True)\n",
    "df_10 = df_10.reset_index(drop=True)\n",
    "df_11=df[df['गुनासो वर्ग']=='अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी '].sample(2863,replace=True)\n",
    "df_11 = df_11.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.concat([df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8,df_9,df_10,df_11],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "nepali_stopwords = set(stopwords.words('nepali'))\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[#\\\\/।(),०-९<<?!,—–’‘:\\u200d]', '', text)\n",
    "    text = text.strip('\"')\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in nepali_stopwords and word.lower() not in english_stopwords]\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    return processed_text\n",
    "\n",
    "df_final['गुनासो'] = df_final['गुनासो'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Labels\n",
    "le = LabelEncoder()\n",
    "df_final['label_encoded'] = le.fit_transform(df_final['गुनासो वर्ग'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(df_final, test_size=0.2, stratify=df_final['label_encoded'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "max_words = 10000\n",
    "max_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['गुनासो'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_data['गुनासो'])\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['गुनासो'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your pre-trained Nepali Word2Vec model\n",
    "word2vec_model_path = \"nepaliW2V_5Million.model\"\n",
    "word2vec_model = Word2Vec.load(word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words and word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model with Word2Vec embeddings\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=50)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=len(np.unique(df_final['label_encoded'])), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788/788 [==============================] - 25s 26ms/step - loss: 1.2821 - accuracy: 0.5701 - val_loss: 0.9680 - val_accuracy: 0.6617\n",
      "Epoch 2/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.9023 - accuracy: 0.6876 - val_loss: 0.8604 - val_accuracy: 0.7030\n",
      "Epoch 3/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.7877 - accuracy: 0.7239 - val_loss: 0.7749 - val_accuracy: 0.7308\n",
      "Epoch 4/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.7199 - accuracy: 0.7463 - val_loss: 0.7653 - val_accuracy: 0.7327\n",
      "Epoch 5/20\n",
      "788/788 [==============================] - 19s 25ms/step - loss: 0.6598 - accuracy: 0.7653 - val_loss: 0.7225 - val_accuracy: 0.7528\n",
      "Epoch 6/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.6147 - accuracy: 0.7793 - val_loss: 0.7318 - val_accuracy: 0.7522\n",
      "Epoch 7/20\n",
      "788/788 [==============================] - 20s 25ms/step - loss: 0.6019 - accuracy: 0.7854 - val_loss: 0.6879 - val_accuracy: 0.7603\n",
      "Epoch 8/20\n",
      "788/788 [==============================] - 20s 25ms/step - loss: 0.5761 - accuracy: 0.7953 - val_loss: 0.7044 - val_accuracy: 0.7696\n",
      "Epoch 9/20\n",
      "788/788 [==============================] - 20s 26ms/step - loss: 0.5609 - accuracy: 0.8005 - val_loss: 0.6604 - val_accuracy: 0.7768\n",
      "Epoch 10/20\n",
      "788/788 [==============================] - 19s 25ms/step - loss: 0.5260 - accuracy: 0.8110 - val_loss: 0.6758 - val_accuracy: 0.7773\n",
      "Epoch 11/20\n",
      "788/788 [==============================] - 19s 25ms/step - loss: 0.5355 - accuracy: 0.8092 - val_loss: 0.6751 - val_accuracy: 0.7833\n",
      "Epoch 12/20\n",
      "788/788 [==============================] - 19s 25ms/step - loss: 0.5170 - accuracy: 0.8159 - val_loss: 0.6898 - val_accuracy: 0.7777\n",
      "Epoch 13/20\n",
      "788/788 [==============================] - 20s 25ms/step - loss: 0.5044 - accuracy: 0.8195 - val_loss: 0.7289 - val_accuracy: 0.7760\n",
      "Epoch 14/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.5081 - accuracy: 0.8200 - val_loss: 0.7009 - val_accuracy: 0.7816\n",
      "Epoch 15/20\n",
      "788/788 [==============================] - 20s 25ms/step - loss: 0.4882 - accuracy: 0.8248 - val_loss: 0.7102 - val_accuracy: 0.7793\n",
      "Epoch 16/20\n",
      "788/788 [==============================] - 20s 25ms/step - loss: 0.4902 - accuracy: 0.8258 - val_loss: 0.7261 - val_accuracy: 0.7771\n",
      "Epoch 17/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4808 - accuracy: 0.8285 - val_loss: 0.7169 - val_accuracy: 0.7854\n",
      "Epoch 18/20\n",
      "788/788 [==============================] - 20s 25ms/step - loss: 0.4663 - accuracy: 0.8321 - val_loss: 0.7455 - val_accuracy: 0.7811\n",
      "Epoch 19/20\n",
      "788/788 [==============================] - 20s 25ms/step - loss: 0.4590 - accuracy: 0.8352 - val_loss: 0.6970 - val_accuracy: 0.7873\n",
      "Epoch 20/20\n",
      "788/788 [==============================] - 20s 25ms/step - loss: 0.4652 - accuracy: 0.8306 - val_loss: 0.7499 - val_accuracy: 0.7793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7d3d4975b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "model.fit(train_padded, train_data['label_encoded'], epochs=epochs, validation_data=(test_padded, test_data['label_encoded']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 3s 9ms/step\n",
      "Classification Report:\n",
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                             अर्थ सबन्धी        0.89      0.77      0.82       572\n",
      "अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी        0.99      0.97      0.98       573\n",
      "                       कर्मचारी सम्वन्धी        0.86      0.78      0.82       573\n",
      "                       खानेपानी सम्बन्धी        0.85      0.72      0.78       573\n",
      "            प्राकृतिक श्रोत/साधन सम्बन्धी       0.95      0.86      0.90       573\n",
      "                    लागु पदार्थ सम्बन्धी        0.99      0.97      0.98       572\n",
      "  वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी        0.32      0.73      0.44       573\n",
      "                 शान्ति सुरक्षा सम्बन्धी        0.91      0.62      0.74       573\n",
      "                सूचना तथा  संचार सम्बन्धी       0.92      0.79      0.85       573\n",
      "          सोधपुछ, सुझाव, प्रशंसा सम्बन्धी       0.76      0.59      0.66       572\n",
      "                    स्वास्थ्यसँग सम्बन्धी       0.87      0.77      0.82       572\n",
      "\n",
      "                                 accuracy                           0.78      6299\n",
      "                                macro avg       0.85      0.78      0.80      6299\n",
      "                             weighted avg       0.85      0.78      0.80      6299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred_probs = model.predict(test_padded)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Decode the encoded labels back to original classes\n",
    "y_true = le.inverse_transform(test_data['label_encoded'])\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_decoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted Class: खानेपानी सम्बन्धी  (Class 3)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"यस महानगरपालिकाको सुनाकोठी नखिपोट नख्खुडोल गोकुल आवास कान्तिपुर कोलोनीका वासिन्दाले दिनरात यो धुंवाको सास कहिलेसम्म फेर्नुपर्ने हो हजुर?\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Class: कर्मचारी सम्वन्धी  (Class 2)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"सुनसरी जिल्ला इटहरी उप महानगर भित्र अवस्थित धरान पुग्ने यो बाटो । कुन ठेकेदारले यो काम गर्दा कति पैसा कुम्ल्याउन पाइने हो र आफ्नो निर्वाचन ताका खर्च भएको करोड उठाउन पाइन्छ भनेर त हैन ?\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted Class: सोधपुछ, सुझाव, प्रशंसा सम्बन्धी (Class 9)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"इटहरीदेखि पूर्वाञ्चल विश्वविद्यालय, माेरङ हुँदै पूर्व जाने बाटाे जुन लालभित्तीमा गएर टुङ्गिन्छ, पिच गर्नुपर्‍याे। बीचमा दुइटा ठूला खाेला छन्, त्यसमा पुल हाल्नुपर्‍याे। याे बाटाेलाई वैकल्पिक राजमार्गकाे रुपमा विकास गर्नुप्रयो।\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted Class: अर्थ सबन्धी  (Class 0)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"मेयरले उपभोक्ता समितिसँग ३० प्रतिशत कमिसन लिएपछि विकासको काम गुणस्तरहीन, दबाब झेल्न नसकेर प्रशासकीय अधिकृतको भागाभाग। बाह्रबिसेका मेयरको मनोमानी- दोहोरीमा रमाइलो गरेको बिलसमेत नगरपालिकाबाटै भुक्तानी गर्न दबाब मेयरले उपभोक्ता समितिसँग ३० प्रतिशत कमिसन लिएपछि विकासको काम गुणस्तरहीन, दबाब झेल्न नसकेर प्रशासकीय अधिकृतको भागाभाग।\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

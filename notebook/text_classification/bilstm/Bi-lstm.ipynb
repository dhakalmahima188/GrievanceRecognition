{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m594.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 KB\u001b[0m \u001b[31m142.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.1 pytz-2024.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/475.2 MB\u001b[0m \u001b[31m11.4 kB/s\u001b[0m eta \u001b[36m11:32:36\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 466, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/lib/python3.10/ssl.py\", line 1303, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/lib/python3.10/ssl.py\", line 1159, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 165, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 205, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 339, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 348, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 215, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 288, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 227, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 299, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 487, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 532, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 214, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 94, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/network/download.py\", line 146, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/cli/progress_bars.py\", line 304, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 512, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/mahima/Documents/GrievanceRecognition/venv/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow[and-cuda]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(\"dataset/updated_text_data.xlsx\")\n",
    "df=pd.read_csv(\"dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=df[df['गुनासो वर्ग']=='वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी ']\n",
    "df_2=df[df['गुनासो वर्ग']=='सोधपुछ, सुझाव, प्रशंसा सम्बन्धी'].sample(2863,replace=True)\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "df_3=df[df['गुनासो वर्ग']=='कर्मचारी सम्वन्धी '].sample(2863,replace=True)\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "df_4=df[df['गुनासो वर्ग']=='स्वास्थ्यसँग सम्बन्धी'].sample(2863,replace=True)\n",
    "df_4 = df_4.reset_index(drop=True)\n",
    "df_5=df[df['गुनासो वर्ग']=='अर्थ सबन्धी '].sample(2863,replace=True)\n",
    "df_5 = df_5.reset_index(drop=True)\n",
    "df_6=df[df['गुनासो वर्ग']=='खानेपानी सम्बन्धी '].sample(2863,replace=True)\n",
    "df_6 = df_6.reset_index(drop=True)\n",
    "df_7=df[df['गुनासो वर्ग']=='सूचना तथा  संचार सम्बन्धी'].sample(2863,replace=True)\n",
    "df_7 = df_7.reset_index(drop=True)\n",
    "df_8=df[df['गुनासो वर्ग']=='शान्ति सुरक्षा सम्बन्धी '].sample(2863,replace=True)\n",
    "df_8 = df_8.reset_index(drop=True)\n",
    "df_9=df[df['गुनासो वर्ग']=='प्राकृतिक श्रोत/साधन सम्बन्धी'].sample(2863,replace=True)\n",
    "df_9 =df_9.reset_index(drop=True)\n",
    "df_10=df[df['गुनासो वर्ग']=='लागु पदार्थ सम्बन्धी '].sample(2863,replace=True)\n",
    "df_10 = df_10.reset_index(drop=True)\n",
    "df_11=df[df['गुनासो वर्ग']=='अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी '].sample(2863,replace=True)\n",
    "df_11 = df_11.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.concat([df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8,df_9,df_10,df_11],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "nepali_stopwords = set(stopwords.words('nepali'))\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[#\\\\/।(),०-९<<?!,—–’‘:\\u200d]', '', text)\n",
    "    text = text.strip('\"')\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in nepali_stopwords and word.lower() not in english_stopwords]\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    return processed_text\n",
    "\n",
    "df_final['गुनासो'] = df_final['गुनासो'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Labels\n",
    "le = LabelEncoder()\n",
    "df_final['label_encoded'] = le.fit_transform(df_final['गुनासो वर्ग'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(df_final, test_size=0.2, stratify=df_final['label_encoded'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "max_words = 10000\n",
    "max_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "\n",
    "tokenizer.fit_on_texts(train_data['गुनासो'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_data['गुनासो'])\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['गुनासो'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your pre-trained Nepali Word2Vec model\n",
    "word2vec_model_path = \"nepaliW2V_5Million.model\"\n",
    "word2vec_model = Word2Vec.load(word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words and word in word2vec_model.wv:\n",
    "        \n",
    "        embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Bi-LSTM model with Word2Vec embeddings\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=50)))\n",
    "model.add(Dense(units=len(np.unique(df_final['label_encoded'])), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 21:34:06.160777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2023-12-10 21:34:07.485570: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f778783d8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-10 21:34:07.485631: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2023-12-10 21:34:07.499683: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-10 21:34:07.663006: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788/788 [==============================] - 25s 24ms/step - loss: 1.1522 - accuracy: 0.6127 - val_loss: 0.9171 - val_accuracy: 0.6771\n",
      "Epoch 2/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.8008 - accuracy: 0.7167 - val_loss: 0.7855 - val_accuracy: 0.7236\n",
      "Epoch 3/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.6866 - accuracy: 0.7566 - val_loss: 0.7106 - val_accuracy: 0.7473\n",
      "Epoch 4/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.6288 - accuracy: 0.7757 - val_loss: 0.6717 - val_accuracy: 0.7644\n",
      "Epoch 5/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.5704 - accuracy: 0.7947 - val_loss: 0.6453 - val_accuracy: 0.7782\n",
      "Epoch 6/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.5375 - accuracy: 0.8020 - val_loss: 0.6633 - val_accuracy: 0.7739\n",
      "Epoch 7/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.5280 - accuracy: 0.8084 - val_loss: 0.6354 - val_accuracy: 0.7843\n",
      "Epoch 8/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4984 - accuracy: 0.8183 - val_loss: 0.6486 - val_accuracy: 0.7790\n",
      "Epoch 9/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4827 - accuracy: 0.8241 - val_loss: 0.6220 - val_accuracy: 0.7904\n",
      "Epoch 10/20\n",
      "788/788 [==============================] - 18s 22ms/step - loss: 0.4801 - accuracy: 0.8251 - val_loss: 0.6304 - val_accuracy: 0.7896\n",
      "Epoch 11/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4604 - accuracy: 0.8312 - val_loss: 0.6223 - val_accuracy: 0.7944\n",
      "Epoch 12/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4593 - accuracy: 0.8325 - val_loss: 0.6405 - val_accuracy: 0.7908\n",
      "Epoch 13/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4555 - accuracy: 0.8345 - val_loss: 0.6307 - val_accuracy: 0.7987\n",
      "Epoch 14/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4430 - accuracy: 0.8375 - val_loss: 0.6398 - val_accuracy: 0.7936\n",
      "Epoch 15/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4425 - accuracy: 0.8375 - val_loss: 0.6713 - val_accuracy: 0.7876\n",
      "Epoch 16/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4477 - accuracy: 0.8356 - val_loss: 0.6587 - val_accuracy: 0.7987\n",
      "Epoch 17/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4256 - accuracy: 0.8456 - val_loss: 0.6347 - val_accuracy: 0.8041\n",
      "Epoch 18/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4152 - accuracy: 0.8483 - val_loss: 0.6610 - val_accuracy: 0.8047\n",
      "Epoch 19/20\n",
      "788/788 [==============================] - 16s 21ms/step - loss: 0.4372 - accuracy: 0.8398 - val_loss: 0.6493 - val_accuracy: 0.8000\n",
      "Epoch 20/20\n",
      "788/788 [==============================] - 16s 20ms/step - loss: 0.4164 - accuracy: 0.8487 - val_loss: 0.6497 - val_accuracy: 0.8057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f79d7f4efb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "model.fit(train_padded, train_data['label_encoded'], epochs=epochs, validation_data=(test_padded, test_data['label_encoded']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 2s 7ms/step\n",
      "Classification Report:\n",
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                             अर्थ सबन्धी        0.91      0.79      0.85       572\n",
      "अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी        0.99      1.00      1.00       573\n",
      "                       कर्मचारी सम्वन्धी        0.91      0.83      0.87       573\n",
      "                       खानेपानी सम्बन्धी        0.94      0.77      0.84       573\n",
      "            प्राकृतिक श्रोत/साधन सम्बन्धी       0.97      0.88      0.92       573\n",
      "                    लागु पदार्थ सम्बन्धी        1.00      0.97      0.98       572\n",
      "  वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी        0.33      0.76      0.46       573\n",
      "                 शान्ति सुरक्षा सम्बन्धी        0.91      0.60      0.72       573\n",
      "                सूचना तथा  संचार सम्बन्धी       0.93      0.85      0.89       573\n",
      "          सोधपुछ, सुझाव, प्रशंसा सम्बन्धी       0.81      0.67      0.74       572\n",
      "                    स्वास्थ्यसँग सम्बन्धी       0.93      0.73      0.82       572\n",
      "\n",
      "                                 accuracy                           0.81      6299\n",
      "                                macro avg       0.88      0.81      0.83      6299\n",
      "                             weighted avg       0.88      0.81      0.83      6299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred_probs = model.predict(test_padded)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Decode the encoded labels back to original classes\n",
    "y_true = le.inverse_transform(test_data['label_encoded'])\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_decoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted Class: कर्मचारी सम्वन्धी  (Class 2)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"यस महानगरपालिकाको सुनाकोठी नखिपोट नख्खुडोल गोकुल आवास कान्तिपुर कोलोनीका वासिन्दाले दिनरात यो धुंवाको सास कहिलेसम्म फेर्नुपर्ने हो हजुर?\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Class: सोधपुछ, सुझाव, प्रशंसा सम्बन्धी (Class 9)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"सुनसरी जिल्ला इटहरी उप महानगर भित्र अवस्थित धरान पुग्ने यो बाटो । कुन ठेकेदारले यो काम गर्दा कति पैसा कुम्ल्याउन पाइने हो र आफ्नो निर्वाचन ताका खर्च भएको करोड उठाउन पाइन्छ भनेर त हैन ?\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted Class: सोधपुछ, सुझाव, प्रशंसा सम्बन्धी (Class 9)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"इटहरीदेखि पूर्वाञ्चल विश्वविद्यालय, माेरङ हुँदै पूर्व जाने बाटाे जुन लालभित्तीमा गएर टुङ्गिन्छ, पिच गर्नुपर्‍याे। बीचमा दुइटा ठूला खाेला छन्, त्यसमा पुल हाल्नुपर्‍याे। याे बाटाेलाई वैकल्पिक राजमार्गकाे रुपमा विकास गर्नुप्रयो।\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Class: अर्थ सबन्धी  (Class 0)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"मेयरले उपभोक्ता समितिसँग ३० प्रतिशत कमिसन लिएपछि विकासको काम गुणस्तरहीन, दबाब झेल्न नसकेर प्रशासकीय अधिकृतको भागाभाग। बाह्रबिसेका मेयरको मनोमानी- दोहोरीमा रमाइलो गरेको बिलसमेत नगरपालिकाबाटै भुक्तानी गर्न दबाब मेयरले उपभोक्ता समितिसँग ३० प्रतिशत कमिसन लिएपछि विकासको काम गुणस्तरहीन, दबाब झेल्न नसकेर प्रशासकीय अधिकृतको भागाभाग।\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"dataset/updated_text_data.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=df[df['गुनासो वर्ग']=='वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी ']\n",
    "df_2=df[df['गुनासो वर्ग']=='सोधपुछ, सुझाव, प्रशंसा सम्बन्धी'].sample(2863,replace=True)\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "df_3=df[df['गुनासो वर्ग']=='कर्मचारी सम्वन्धी '].sample(2863,replace=True)\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "df_4=df[df['गुनासो वर्ग']=='स्वास्थ्यसँग सम्बन्धी'].sample(2863,replace=True)\n",
    "df_4 = df_4.reset_index(drop=True)\n",
    "df_5=df[df['गुनासो वर्ग']=='अर्थ सबन्धी '].sample(2863,replace=True)\n",
    "df_5 = df_5.reset_index(drop=True)\n",
    "df_6=df[df['गुनासो वर्ग']=='खानेपानी सम्बन्धी '].sample(2863,replace=True)\n",
    "df_6 = df_6.reset_index(drop=True)\n",
    "df_7=df[df['गुनासो वर्ग']=='सूचना तथा  संचार सम्बन्धी'].sample(2863,replace=True)\n",
    "df_7 = df_7.reset_index(drop=True)\n",
    "df_8=df[df['गुनासो वर्ग']=='शान्ति सुरक्षा सम्बन्धी '].sample(2863,replace=True)\n",
    "df_8 = df_8.reset_index(drop=True)\n",
    "df_9=df[df['गुनासो वर्ग']=='प्राकृतिक श्रोत/साधन सम्बन्धी'].sample(2863,replace=True)\n",
    "df_9 =df_9.reset_index(drop=True)\n",
    "df_10=df[df['गुनासो वर्ग']=='लागु पदार्थ सम्बन्धी '].sample(2863,replace=True)\n",
    "df_10 = df_10.reset_index(drop=True)\n",
    "df_11=df[df['गुनासो वर्ग']=='अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी '].sample(2863,replace=True)\n",
    "df_11 = df_11.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.concat([df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8,df_9,df_10,df_11],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "nepali_stopwords = set(stopwords.words('nepali'))\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[#\\\\/।(),०-९<<?!,—–’‘:\\u200d]', '', text)\n",
    "    text = text.strip('\"')\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in nepali_stopwords and word.lower() not in english_stopwords]\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    return processed_text\n",
    "\n",
    "df_final['गुनासो'] = df_final['गुनासो'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Labels\n",
    "le = LabelEncoder()\n",
    "df_final['label_encoded'] = le.fit_transform(df_final['गुनासो वर्ग'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(df_final, test_size=0.2, stratify=df_final['label_encoded'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "max_words = 10000\n",
    "max_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "\n",
    "tokenizer.fit_on_texts(train_data['गुनासो'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_data['गुनासो'])\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['गुनासो'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your pre-trained Nepali Word2Vec model\n",
    "word2vec_model_path = \"nepaliW2V_5Million.model\"\n",
    "word2vec_model = Word2Vec.load(word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words and word in word2vec_model.wv:\n",
    "        \n",
    "        embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Bi-LSTM model with Word2Vec embeddings\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=50)))\n",
    "model.add(Dense(units=len(np.unique(df_final['label_encoded'])), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 21:34:06.160777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2023-12-10 21:34:07.485570: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f778783d8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-10 21:34:07.485631: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2023-12-10 21:34:07.499683: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-10 21:34:07.663006: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788/788 [==============================] - 25s 24ms/step - loss: 1.1522 - accuracy: 0.6127 - val_loss: 0.9171 - val_accuracy: 0.6771\n",
      "Epoch 2/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.8008 - accuracy: 0.7167 - val_loss: 0.7855 - val_accuracy: 0.7236\n",
      "Epoch 3/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.6866 - accuracy: 0.7566 - val_loss: 0.7106 - val_accuracy: 0.7473\n",
      "Epoch 4/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.6288 - accuracy: 0.7757 - val_loss: 0.6717 - val_accuracy: 0.7644\n",
      "Epoch 5/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.5704 - accuracy: 0.7947 - val_loss: 0.6453 - val_accuracy: 0.7782\n",
      "Epoch 6/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.5375 - accuracy: 0.8020 - val_loss: 0.6633 - val_accuracy: 0.7739\n",
      "Epoch 7/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.5280 - accuracy: 0.8084 - val_loss: 0.6354 - val_accuracy: 0.7843\n",
      "Epoch 8/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4984 - accuracy: 0.8183 - val_loss: 0.6486 - val_accuracy: 0.7790\n",
      "Epoch 9/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4827 - accuracy: 0.8241 - val_loss: 0.6220 - val_accuracy: 0.7904\n",
      "Epoch 10/20\n",
      "788/788 [==============================] - 18s 22ms/step - loss: 0.4801 - accuracy: 0.8251 - val_loss: 0.6304 - val_accuracy: 0.7896\n",
      "Epoch 11/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4604 - accuracy: 0.8312 - val_loss: 0.6223 - val_accuracy: 0.7944\n",
      "Epoch 12/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4593 - accuracy: 0.8325 - val_loss: 0.6405 - val_accuracy: 0.7908\n",
      "Epoch 13/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4555 - accuracy: 0.8345 - val_loss: 0.6307 - val_accuracy: 0.7987\n",
      "Epoch 14/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4430 - accuracy: 0.8375 - val_loss: 0.6398 - val_accuracy: 0.7936\n",
      "Epoch 15/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4425 - accuracy: 0.8375 - val_loss: 0.6713 - val_accuracy: 0.7876\n",
      "Epoch 16/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4477 - accuracy: 0.8356 - val_loss: 0.6587 - val_accuracy: 0.7987\n",
      "Epoch 17/20\n",
      "788/788 [==============================] - 19s 24ms/step - loss: 0.4256 - accuracy: 0.8456 - val_loss: 0.6347 - val_accuracy: 0.8041\n",
      "Epoch 18/20\n",
      "788/788 [==============================] - 18s 23ms/step - loss: 0.4152 - accuracy: 0.8483 - val_loss: 0.6610 - val_accuracy: 0.8047\n",
      "Epoch 19/20\n",
      "788/788 [==============================] - 16s 21ms/step - loss: 0.4372 - accuracy: 0.8398 - val_loss: 0.6493 - val_accuracy: 0.8000\n",
      "Epoch 20/20\n",
      "788/788 [==============================] - 16s 20ms/step - loss: 0.4164 - accuracy: 0.8487 - val_loss: 0.6497 - val_accuracy: 0.8057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f79d7f4efb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "model.fit(train_padded, train_data['label_encoded'], epochs=epochs, validation_data=(test_padded, test_data['label_encoded']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 2s 7ms/step\n",
      "Classification Report:\n",
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                             अर्थ सबन्धी        0.91      0.79      0.85       572\n",
      "अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी        0.99      1.00      1.00       573\n",
      "                       कर्मचारी सम्वन्धी        0.91      0.83      0.87       573\n",
      "                       खानेपानी सम्बन्धी        0.94      0.77      0.84       573\n",
      "            प्राकृतिक श्रोत/साधन सम्बन्धी       0.97      0.88      0.92       573\n",
      "                    लागु पदार्थ सम्बन्धी        1.00      0.97      0.98       572\n",
      "  वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी        0.33      0.76      0.46       573\n",
      "                 शान्ति सुरक्षा सम्बन्धी        0.91      0.60      0.72       573\n",
      "                सूचना तथा  संचार सम्बन्धी       0.93      0.85      0.89       573\n",
      "          सोधपुछ, सुझाव, प्रशंसा सम्बन्धी       0.81      0.67      0.74       572\n",
      "                    स्वास्थ्यसँग सम्बन्धी       0.93      0.73      0.82       572\n",
      "\n",
      "                                 accuracy                           0.81      6299\n",
      "                                macro avg       0.88      0.81      0.83      6299\n",
      "                             weighted avg       0.88      0.81      0.83      6299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred_probs = model.predict(test_padded)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Decode the encoded labels back to original classes\n",
    "y_true = le.inverse_transform(test_data['label_encoded'])\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_decoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted Class: कर्मचारी सम्वन्धी  (Class 2)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"यस महानगरपालिकाको सुनाकोठी नखिपोट नख्खुडोल गोकुल आवास कान्तिपुर कोलोनीका वासिन्दाले दिनरात यो धुंवाको सास कहिलेसम्म फेर्नुपर्ने हो हजुर?\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Class: सोधपुछ, सुझाव, प्रशंसा सम्बन्धी (Class 9)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"सुनसरी जिल्ला इटहरी उप महानगर भित्र अवस्थित धरान पुग्ने यो बाटो । कुन ठेकेदारले यो काम गर्दा कति पैसा कुम्ल्याउन पाइने हो र आफ्नो निर्वाचन ताका खर्च भएको करोड उठाउन पाइन्छ भनेर त हैन ?\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted Class: सोधपुछ, सुझाव, प्रशंसा सम्बन्धी (Class 9)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"इटहरीदेखि पूर्वाञ्चल विश्वविद्यालय, माेरङ हुँदै पूर्व जाने बाटाे जुन लालभित्तीमा गएर टुङ्गिन्छ, पिच गर्नुपर्‍याे। बीचमा दुइटा ठूला खाेला छन्, त्यसमा पुल हाल्नुपर्‍याे। याे बाटाेलाई वैकल्पिक राजमार्गकाे रुपमा विकास गर्नुप्रयो।\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Class: अर्थ सबन्धी  (Class 0)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"मेयरले उपभोक्ता समितिसँग ३० प्रतिशत कमिसन लिएपछि विकासको काम गुणस्तरहीन, दबाब झेल्न नसकेर प्रशासकीय अधिकृतको भागाभाग। बाह्रबिसेका मेयरको मनोमानी- दोहोरीमा रमाइलो गरेको बिलसमेत नगरपालिकाबाटै भुक्तानी गर्न दबाब मेयरले उपभोक्ता समितिसँग ३० प्रतिशत कमिसन लिएपछि विकासको काम गुणस्तरहीन, दबाब झेल्न नसकेर प्रशासकीय अधिकृतको भागाभाग।\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

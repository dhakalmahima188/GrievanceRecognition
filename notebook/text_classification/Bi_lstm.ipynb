{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N97cqZUnuIim"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "sfUv6wcLuVR_",
        "outputId": "e2bc78a4-27de-45ea-b3de-8b2b0a14fac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/Adataset_new - final.csv\")"
      ],
      "metadata": {
        "id": "oTEh6a67unjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['category'].value_counts()"
      ],
      "metadata": {
        "id": "he6xABi6vFhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    \"वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी\",\n",
        "    \"पार्किङ्ग तथा मेट्रो पुलिस\",\n",
        "    \"सोधपुछ, सुझाव, प्रशंसा सम्बन्धी\",\n",
        "    \"कर्मचारी सम्वन्धी\",\n",
        "    \"खानेपानी\",\n",
        "    \"अर्थ सबन्धी\",\n",
        "    \"शान्ति सुरक्षा\",\n",
        "    \"स्वास्थ्यसँग सम्बन्धी\",\n",
        "    \"सूचना तथा संचार\",\n",
        "    \"फोहोरमैला व्यवस्थापन\",\n",
        "    \"निर्माण कार्य सम्बन्धी\",\n",
        "    \"प्राकृतिक श्रोत/साधन सम्बन्धी\",\n",
        "    \"लागु पदार्थ सम्बन्धी\",\n",
        "    \"अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "CNhnDZ4RzlXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la5k6SBbuIiq"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_final = pd.DataFrame()  # Initialize an empty DataFrame to hold the final result\n",
        "\n",
        "for category in categories:\n",
        "    # Select rows matching the current category\n",
        "    temp_df = df[df['category'] == category]\n",
        "\n",
        "    # If the category requires sampling, perform sampling; otherwise, proceed as is\n",
        "    if category != 'वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी':  # Adjust this condition as needed\n",
        "        temp_df = temp_df.sample(2863, replace=True).reset_index(drop=True)\n",
        "\n",
        "    df_final = pd.concat([df_final, temp_df], axis=0)\n",
        "\n",
        "# Optionally, reset the index of the final DataFrame\n",
        "df_final = df_final.reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.value_counts()"
      ],
      "metadata": {
        "id": "VY-itGPSd03t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "dDz0R18c1ezj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de676f9-7761-482b-86e5-cbc25b1a5e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "EE-yi96x1eQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')  # For English stopwords\n",
        "print(stop_words)  # This will print the list of English stopwords\n"
      ],
      "metadata": {
        "id": "VY3NXsfr1R2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805a5872-b7ab-43ff-9d56-1aab59aad73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "DrIkK7z-1_xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd1836ca-e541-4b04-9898-eab425425723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CsoBkmhuIir"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "nepali_stopwords = set(stopwords.words('nepali'))\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('[#\\\\/।(),०-९<<?!,—–’‘:\\u200d]', '', text)\n",
        "    text = text.strip('\"')\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in nepali_stopwords and word.lower() not in english_stopwords]\n",
        "    processed_text = ' '.join(filtered_words)\n",
        "    return processed_text\n",
        "\n",
        "# df_final['complain'] = df_final['complain'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzbNrZ4iuIis"
      },
      "outputs": [],
      "source": [
        "# Encoding Labels\n",
        "le = LabelEncoder()\n",
        "df_final['label_encoded'] = le.fit_transform(df_final['category'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCv5DRx6uIis"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_data, test_data = train_test_split(df_final, test_size=0.2, stratify=df_final['label_encoded'], random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzvualRWuIis"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text data\n",
        "max_words = 10000\n",
        "max_length = 100\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_data['complain'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN9mvtYEuIit"
      },
      "outputs": [],
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_data['complain'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['complain'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcw4uay7uIit"
      },
      "outputs": [],
      "source": [
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69u5SliNuIit"
      },
      "outputs": [],
      "source": [
        "# Load your pre-trained Nepali Word2Vec model\n",
        "word2vec_model_path = \"/content/drive/MyDrive/nepaliW2V_5Million.model\"\n",
        "word2vec_model = Word2Vec.load(word2vec_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOkdMUzsuIit"
      },
      "outputs": [],
      "source": [
        "# Create an embedding matrix\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_words and word in word2vec_model.wv:\n",
        "\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAiBaH8tuIiu"
      },
      "outputs": [],
      "source": [
        "# Build the Bi-LSTM model with Word2Vec embeddings\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(Bidirectional(LSTM(units=100, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(units=50)))\n",
        "model.add(Dense(units=len(np.unique(df_final['label_encoded'])), activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXdzXcMouIiu"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 20\n",
        "history=model.fit(train_padded, train_data['label_encoded'], epochs=epochs, validation_data=(test_padded, test_data['label_encoded']))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = range(1, 21)\n",
        "accuracy = [0.7126, 0.8289, 0.8817, 0.9101, 0.9331, 0.9414, 0.9560, 0.9592, 0.9608, 0.9721, 0.9692, 0.9736, 0.9756, 0.9736, 0.9796, 0.9785, 0.9767, 0.9766, 0.9819, 0.9820]\n",
        "f1_score = [0.7909, 0.8445, 0.8770, 0.9028, 0.9030, 0.9176, 0.9168, 0.9239, 0.9314, 0.9292, 0.9246, 0.9337, 0.9310, 0.9299, 0.9374, 0.9361, 0.9176, 0.9390, 0.9301, 0.9367]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(epochs, accuracy, label='Accuracy', color='tab:blue', marker='o')\n",
        "plt.plot(epochs, f1_score, label='F1-score', color='tab:orange', linestyle='--', marker='o')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metrics')\n",
        "plt.title('Accuracy and F1-score over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A8yPHuxbeWo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = range(1, 21)\n",
        "training_loss = [0.9150, 0.5152, 0.3542, 0.2689, 0.1997, 0.1737, 0.1318, 0.1159, 0.1162, 0.0836, 0.0890, 0.0764, 0.0688, 0.0777, 0.0573, 0.0582, 0.0666, 0.0636, 0.0488, 0.0510]\n",
        "validation_loss = [0.6406, 0.4726, 0.3876, 0.3118, 0.3157, 0.2692, 0.2726, 0.2639, 0.2527, 0.2728, 0.2890, 0.2729, 0.2830, 0.2836, 0.2871, 0.2881, 0.3491, 0.2676, 0.3001, 0.2996]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(epochs, training_loss, label='Training Loss', color='tab:blue', marker='o')\n",
        "plt.plot(epochs, validation_loss, label='Validation Loss', color='tab:orange', linestyle='--', marker='o')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Bq8s1lcCegVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8fIWdURGasta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation accuracies\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9hce2DSra3b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSh83H60ayl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1kGeZk485a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwiW44tluIiu"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(test_padded)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Decode the encoded labels back to original classes\n",
        "y_true = le.inverse_transform(test_data['label_encoded'])\n",
        "y_pred_decoded = le.inverse_transform(y_pred)\n",
        "\n",
        "# Print the classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_decoded))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/model.h5\")"
      ],
      "metadata": {
        "id": "0o6bI5qRb29m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load_model('/content/drive/MyDrive/model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "qjYaWJHUcqUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyk49Y5wuIiu"
      },
      "outputs": [],
      "source": [
        "# New Nepali text\n",
        "new_text = \"यस महानगरपालिकाको सुनाकोठी नखिपोट नख्खुडोल गोकुल आवास कान्तिपुर कोलोनीका वासिन्दाले दिनरात यो धुंवाको सास कहिलेसम्म फेर्नुपर्ने हो हजुर?\"\n",
        "\n",
        "# Preprocess the new text\n",
        "processed_text = preprocess_text(new_text)\n",
        "\n",
        "# Tokenize and pad the sequence\n",
        "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Make predictions\n",
        "new_pred_probs = loaded_model.predict(new_padded)\n",
        "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
        "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
        "\n",
        "# Print the result\n",
        "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EETWMdGiuIiu"
      },
      "outputs": [],
      "source": [
        "# New Nepali text\n",
        "new_text = \"सुनसरी जिल्ला इटहरी उप महानगर भित्र अवस्थित धरान पुग्ने यो बाटो । कुन ठेकेदारले यो काम गर्दा कति पैसा कुम्ल्याउन पाइने हो र आफ्नो निर्वाचन ताका खर्च भएको करोड उठाउन पाइन्छ भनेर त हैन ?\"\n",
        "\n",
        "# Preprocess the new text\n",
        "processed_text = preprocess_text(new_text)\n",
        "\n",
        "# Tokenize and pad the sequence\n",
        "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Make predictions\n",
        "new_pred_probs = model.predict(new_padded)\n",
        "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
        "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
        "\n",
        "# Print the result\n",
        "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmtlct7muIiv"
      },
      "outputs": [],
      "source": [
        "# New Nepali text\n",
        "new_text = \"काठमाडौं मा मेलम्ची को पानी बितरन्ण भयो भन्छ । हामी मा अहीपुग्ने कहिले हो मेरो मा पानी न आहेक्क आजको दिन टक्काइ ४० दिन भयो । खानी पानी कार्यल मा भनो भने पाले ले आजै दिन्छ भन्छ हामी लाई फर्काइ दिन्छ अब भन्नुस् कति दिन यो महङ्गो मा पानी किनेर जिबन याबन गर्ने । हजुर हरु लाई के लाग्छ के हामी पानी बिना को जिबन जिउन सकिन्छ त ।कृपया हाम्रो समस्य बुझीदिन्छ कि भनेर हजुर सम्म यो गुनासो पठाउन बात्य भएको छु ।\"\n",
        "\n",
        "# Preprocess the new text\n",
        "processed_text = preprocess_text(new_text)\n",
        "\n",
        "# Tokenize and pad the sequence\n",
        "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Make predictions\n",
        "new_pred_probs = model.predict(new_padded)\n",
        "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
        "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
        "\n",
        "# Print the result\n",
        "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPGuxUQPuIiv"
      },
      "outputs": [],
      "source": [
        "# New Nepali text\n",
        "new_text = \"नगरिक ऐप नागरिकता संशोधन\"\n",
        "\n",
        "# Preprocess the new text\n",
        "processed_text = preprocess_text(new_text)\n",
        "\n",
        "# Tokenize and pad the sequence\n",
        "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Make predictions\n",
        "new_pred_probs = model.predict(new_padded)\n",
        "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
        "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
        "\n",
        "# Print the result\n",
        "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nepali-stemmer"
      ],
      "metadata": {
        "id": "SNap9xtWBvpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef47ebc8-3c90-4872-92ce-cf6906e04051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nepali-stemmer\n",
            "  Downloading nepali_stemmer-0.0.2-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nepali-stemmer\n",
            "Successfully installed nepali-stemmer-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"काठमाडौं मा मेलम्ची को पानी बितरन्ण भयो भन्छ । हामी मा अहीपुग्ने कहिले हो मेरो मा पानी न आहेक्क आजको दिन टक्काइ ४० दिन भयो । खानी पानी कार्यल मा भनो भने पाले ले आजै दिन्छ भन्छ हामी लाई फर्काइ दिन्छ अब भन्नुस् कति दिन यो महङ्गो मा पानी किनेर जिबन याबन गर्ने । हजुर हरु लाई के लाग्छ के हामी पानी बिना को जिबन जिउन सकिन्छ त ।कृपया हाम्रो समस्य बुझीदिन्छ कि भनेर हजुर सम्म यो गुनासो पठाउन बात्य भएको छु ।\"\n"
      ],
      "metadata": {
        "id": "JitWEgpX3Ijh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nepali_stemmer.stemmer import NepStemmer\n",
        "nepstem = NepStemmer()\n",
        "nepstem.stem(new_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "F6x-6Y3J3NFT",
        "outputId": "81654fe4-ff1a-43d9-a021-9dfa7e385606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'काठमाडौं मा मेलम्ची को पानी बितरन्ण भयो भन्छ । हामी मा अहीपुग्ने कहिले हो मेरो मा पानी न आहेक्क आज को दिन टक्काइ ४० दिन भयो । खानी पानी कार्यल मा भनो भने पाले ले आजै दिन्छ भन्छ हामी लाई फर्काइ दिन्छ अब भन्नुस् कति दिन यो महङ्गो मा पानी किनेर जिबन याबन गर्ने । हजुर हरु लाई के लाग्छ के हामी पानी बिना को जिबन जिउन सकिन्छ त ।कृपया हाम्रो समस्य बुझीदिन्छ कि भनेर हजुर सम्म यो गुनासो पठाउन बात्य भए को छु ।'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "preprocess_text(new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wnsUYQIo3V19",
        "outputId": "6af1a3f1-3995-4613-fafe-92fb9e6e98e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'काठमाडौं मेलम्ची पानी बितरन्ण भयो भन्छ हामी अहीपुग्ने कहिले पानी आहेक्क दिन टक्काइ दिन भयो खानी पानी कार्यल भनो पाले आजै दिन्छ भन्छ हामी फर्काइ दिन्छ भन्नुस् कति दिन महङ्गो पानी किनेर जिबन याबन हजुर हरु लाग्छ हामी पानी बिना जिबन जिउन सकिन्छ हाम्रो समस्य बुझीदिन्छ भनेर हजुर गुनासो पठाउन बात्य'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snowballstemmer\n",
        "nepali_stemmer = snowballstemmer.NepaliStemmer()\n",
        "y=new_text.split(\" \")\n",
        "print(y)\n",
        "nepali_stemmer.stemWords(y)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr5a_1VV3qFc",
        "outputId": "30aba30c-f167-4097-d32b-57d0e4cb75f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['काठमाडौं', 'मा', 'मेलम्ची', 'को', 'पानी', 'बितरन्ण', 'भयो', 'भन्छ', '।', 'हामी', 'मा', 'अहीपुग्ने', 'कहिले', 'हो', 'मेरो', 'मा', 'पानी', 'न', 'आहेक्क', 'आजको', 'दिन', 'टक्काइ', '४०', 'दिन', 'भयो', '।', 'खानी', 'पानी', 'कार्यल', 'मा', 'भनो', 'भने', 'पाले', 'ले', 'आजै', 'दिन्छ', 'भन्छ', 'हामी', 'लाई', 'फर्काइ', 'दिन्छ', 'अब', 'भन्नुस्', 'कति', 'दिन', 'यो', 'महङ्गो', 'मा', 'पानी', 'किनेर', 'जिबन', 'याबन', 'गर्ने', '।', 'हजुर', 'हरु', 'लाई', 'के', 'लाग्छ', 'के', 'हामी', 'पानी', 'बिना', 'को', 'जिबन', 'जिउन', 'सकिन्छ', 'त', '।कृपया', 'हाम्रो', 'समस्य', 'बुझीदिन्छ', 'कि', 'भनेर', 'हजुर', 'सम्म', 'यो', 'गुनासो', 'पठाउन', 'बात्य', 'भएको', 'छु', '।']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['काठमाडौं',\n",
              " '',\n",
              " 'मेलम्ची',\n",
              " '',\n",
              " 'पानी',\n",
              " 'बितरन्ण',\n",
              " '',\n",
              " 'भन्',\n",
              " '।',\n",
              " 'हामी',\n",
              " '',\n",
              " 'अहीपुग्',\n",
              " 'कहि',\n",
              " 'हो',\n",
              " 'मेरो',\n",
              " '',\n",
              " 'पानी',\n",
              " 'न',\n",
              " 'आहेक्क',\n",
              " 'आज',\n",
              " 'दिन',\n",
              " 'टक्काइ',\n",
              " '४०',\n",
              " 'दिन',\n",
              " '',\n",
              " '।',\n",
              " 'खानी',\n",
              " 'पानी',\n",
              " 'कार्यल',\n",
              " '',\n",
              " 'भनो',\n",
              " 'भ',\n",
              " 'पा',\n",
              " '',\n",
              " 'आजै',\n",
              " 'द',\n",
              " 'भन्',\n",
              " 'हामी',\n",
              " '',\n",
              " 'फर्काइ',\n",
              " 'द',\n",
              " 'अब',\n",
              " 'भन्नुस्',\n",
              " 'कति',\n",
              " 'दिन',\n",
              " '',\n",
              " 'महङ्गो',\n",
              " '',\n",
              " 'पानी',\n",
              " 'किनेर',\n",
              " 'जिबन',\n",
              " 'याबन',\n",
              " 'गर्',\n",
              " '।',\n",
              " 'हजुर',\n",
              " '',\n",
              " '',\n",
              " 'के',\n",
              " 'लाग्',\n",
              " 'के',\n",
              " 'हामी',\n",
              " 'पानी',\n",
              " 'बिना',\n",
              " '',\n",
              " 'जिबन',\n",
              " 'जिउन',\n",
              " 'सक',\n",
              " 'त',\n",
              " '।कृपया',\n",
              " 'हाम्रो',\n",
              " 'समस्य',\n",
              " 'बुझीद',\n",
              " '',\n",
              " 'भनेर',\n",
              " 'हजुर',\n",
              " 'सम्म',\n",
              " '',\n",
              " 'गुनासो',\n",
              " 'पठाउन',\n",
              " 'बात्य',\n",
              " 'भ',\n",
              " '',\n",
              " '।']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snowballstemmer\n",
        "nepali_stemmer = snowballstemmer.NepaliStemmer()\n",
        "nepali_stemmer.stemWords([\"नेपालीको\", \"आफ्नै\", \"स्तेम्मेर\"])\n",
        "# ['नेपाली', 'आफ्नै', 'स्तेम्मेर']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3nEZo-y_U7J",
        "outputId": "eab53c6a-f307-4e9a-8086-bde65d4720ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['नेपाली', 'आफ्नै', 'स्तेम्मेर']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "orhK9-Th_U96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmHy29Bu_Djo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}